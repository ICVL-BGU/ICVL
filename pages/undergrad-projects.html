<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Senior projects</title>
    <base href="/ICVL/">
    <link rel="stylesheet" href="css/all.css">

</head>
<body>
    <div id="header-placeholder"></div>
    <div class="main-container">
        <main id="content">

<div class="senior-projects">
<h1>Undergrad Projects</h1>
<p>The iCVL offers various undergrad projects for teams of 2-4 students in Computer Science (marked CS, suggested and available each semester) or Software Engineering (marked SE, suggested and available once a year). The list below is updated once a year and most of the time is kept up-to-date and contains projects that are open. (Recent ongoing or completed projects are listed separately at the bottom of the page.) If you prefer to work individually, you are welcome to speak to <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a> to adjust any of the suggested projects or assign other ad hoc (typically research) projects.</p>

<h3>A robotic fish tank for the study of animal behavior (CS,SE)</h3>
<p>Computational Vision could be a wonderful tool for the study of animal behavior. Indeed, instead of allocating human observers to monitor the animals and document their behavior, why not placing cameras and other sensors to do so? In this project, we will do exactly that to study unique behavioral patterns by goldfish and archer fish. We will develop high-tech robotic fish tanks that automate all sorts of measurements and actions related to the study of fish behavior, vision, and navigation, such as monitoring the fish pose in the task (location and direction), controlling multiple feeders that reward the fish in certain situations, and manipulating the task itself. This project is part of a larger research program in collaboration with Ronen Segev’s lab in the Life Sciences department and will result in the first-ever fully automatic fish tank that trains and observes fish behavior all by itself to answer scientific questions at the forefront of neuroscience. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Eye movements and visual acuity (CS)</h3>
<p>While we are not fully aware of this, humans execute rapid eye movements between locations well over 100,000 times a day. Even when we fixate, our eyes continue to move in various micro patterns in what’s called fixation eye movements. In this project, we’ll experiment with all of these types of movements under different conditions of stimulus sharpness. Using a state-of-the-art eye tracker in the iCVL, we’ll collect data, analyze it, and attempt to obtain interesting insights. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>A scale-space representation for eye movement scan paths (CS)</h3>
<p>As mentioned above, humans execute rapid eye movements between locations well over 100,000 times a day. These movements consist of transitions of many different amplitudes and directions, grossly classified as fixational eye movements and saccades, and most eye trackers indeed apply this classification in a proprietary fashion. In this research project, we will build a system and a GUI that allows presenting and studying the raw scan paths more closely, and in particular, to override the eye tracker’s decision-making in a parametric way, seeking a scale-space representation for scan paths. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Mobile Gazer – A mobile phone eye tracker (CS,SE)</h3>
<p>Tracking gaze is now ubiquitous and various eye trackers are available in different forms, including head-mounted trackers, remote trackers, and even portable trackers mounted on glasses. In this project, we will seek to develop a portable remote eye tracker on a smartphone such that one’s gaze can be monitored while using the phone. This capacity, implemented with the phone’s front camera, can then be used for a multitude of applications, from entertainment (gaming), through biometrics, to assisting the handicapped, and we will explore at least one such application in the projects too. Smartphone programming or computer vision skills are a plus but not mandatory. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Hyperspectral vision (CS, SE conditional)</h3>
<p>While color vision allows humans and other primates to see the world in amazing detail, light in the visible spectrum carries more information than just “Red, green and blue”, advanced “Hyperspectral” cameras allow us to explore the world around us in greater detail, revealing the hidden colors that lie beyond the capabilities of human vision. In this project, we will use advanced hyperspectral imaging equipment to observe natural scenes and study their properties. Several different projects may be available under this topic. <strong>For details please contact:</strong> <a href="mailto:me@boazarad.com" role="link">Dr. Boaz Arad</a> or <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>A Scarecrow drone (or… the Pigeon buster) (SE)</h3>
<p>A practical problem in home maintenance, as well as in agricultural settings, is driving away harmful birds that either litter (e.g., pigeons at home) or damage yield (e.g., Pelicans or Grouses in the field). We wish to harness the power of drones, as autonomous flying agents equipped with proper sensing, to detect the presence of these creatures and fly closer to them to scare them away. In this project, we will develop such a drone system with a certain degree of autonomy for monitoring a designated property (house or field) and protect it. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Image extrapolator – predicting the unseen (CS)</h3>
<p>What if we could predict reliably how an image (or any visual fragment) should look like beyond its borders? In this project, we will implement both existing and novel methods, as well as mechanisms to evaluate these predictions. Most techniques will be data-driven (using machine learning tools). <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Anamorphic synthesizer (SE)</h3>
<p>If you ever been to Las Vegas, you may have visited the SLS hotel to see the <a href="https://www.youtube.com/watch?v=EWvQ0cgQwXQ" role="link">stunning 3D show over the center bar</a>. This form of <a href="https://en.wikipedia.org/wiki/Anamorphosis" role="link">anamorphosis</a> is indeed particularly appealing and realistic for its dynamic nature and is not terribly difficult to synthesize automatically from a dynamic 3D model of the scene. In this project, we will implement such a system for any given display device and any given vantage point. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Bird collision alert and monitoring systems (CS,SE)</h3>
<p>During the year, millions of birds migrate, occupy the sky, and roost in stopover sites, day and night. Collisions with man-made structures (in particular, airplanes and helicopters) result in the deaths of many birds, occasional serious harm to people, and damages of millions of dollars each year. To reduce negative impacts on wildlife and human suffering, it is essential to monitor, track, alert, and actively engage to reduce the chances of collisions. In this joint project (or set of projects) with the Israeli Air Force (IAF), we aim to construct a (1) computer vision pipeline to monitor, track, and alert air traffic control towers about bird hazards in roosts and stopover sites, and (2) develop an automated system to engage with the threat and scare the birds away (e.g., with a drone). <strong>For details please contact:</strong> <a href="mailto:tidharlevari@gmail.com" role="link">Dr. Tidhar Lev Ari</a>, IAF, <a href="mailto:yotam.orchan@mail.huji.ac.il" role="link">Dr. Yotam Orchan</a>, IAF, or <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Individual Animal Identification for Behavioral Research in the Wild (CS,SE)</h3>
<p>Animal behavior  is often explored in controlled lab environments, where individual subjects are easily identified for proper data collection and analysis. This methodology is much more challenging in the wild, where individual subjects behave freely, not confined to any specific region in their habitat, and often escape continuous tracking. Moreover, in many species it is impossible or illegal to mark individuals, although they appear very similar to human observers. In this project we will explore and develop methods to automatically identify individual subjects from images ande videos of such species, to facilitate continuous behavioral research in the wild. Planned species for this project include various reptiles for studies in exotic parts of the globe <strong>For details please contact:</strong> <a href="mailto:bouskila@bgu.ac.il" role="link">Pro. Amos Bouskila</a>, Life Sciences Department, or <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Augmented reality for architectural design (CS,SE)</h3>
<p>Architects and interior designers design draw their plans as 2D sketches. Customers, however, are not able to visualize their house from these drawings and desire 3D simulations of their house. How wonderful could it be if this gap could be bridged directly from the 2D drawing using one’s smartphone? In this project, we will implement a simple augmented reality system that observes simple 2D drawings of one-floor houses and in real-time augments the proper 3D structure over it. Additional features over the basic capacity will be considered along the project. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Deep learning for Shape from Shading (CS)</h3>
<p>Ill-posed inverse problems in vision (or other domains) are one-to-many problems where the I/O mapping is under-determined. In some very basic sense, the vision problem as a whole is one grand inverse problem. For decades now the computer vision community has been addressing such problems using regularized optimization – a fancy term to say more constraints are needed to single out certain solutions over others. Could deep learning, the new king in town, do better? In this project, we will study some literature and experiment with one of the most basic inverse problems in vision, where the shape of objects is to be inferred from their images. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h2>Ongoing Projects (assigned and running)</h2>

<h3>Computational and Robotic Art (CS,SE)</h3>
<p>Computation is not the first thing that comes to mind in the context of artistic expression. But sometimes, art cannot be created without computation, and sometimes computation facilitates new versions of existing art. Both paths are another manifestation of the emerging field of <a href="https://en.wikipedia.org/wiki/Digital_humanities" role="link">digital humanities</a>. In this class of projects, we will explore such combinations, possibly with the collaboration of the <a href="https://in.bgu.ac.il/en/humsos/art/Pages/default.aspx" role="link">Department of the Arts at BGU</a>. A first project will try to replicate Petros Vrellis’ “<a href="http://artof01.com/vrellis/works/knit.html" role="link">A New Way to Knit</a>”, though not necessarily for a circular frame but for an arbitrary convex one. Depending on the size of the team and the scope and scale of the project, a physical implementation using a robotic arm will be included also. Additional projects may include 3D printing as well. Fun is guaranteed! <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<!--    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX -->

<h2>Recently completed projects</h2>

<h3>Robotic polygonal puzzle solver (CS,SE)</h3>
<p>The <a href="https://icvl.cs.bgu.ac.il" role="link">iCVL</a> lab has been <a href="./pages/researches/Square-Jigsaw-Puzzle-Solving.html" role="link">pushing the abilities to solve visual puzzles to new fronts</a>, including implementations of robotic puzzle solving for what is known as “square puzzles” (puzzles of square pieces). In this project, we will explore the automatic acquisition and solution of simple physical polygonal puzzles, bringing us closer to real-life applications like archeology (where pieces rarely are square). <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Augmented reality Geo-locator app (CS,SE)</h3>
<p>How often does it happen to you that you scan the scenery, observe far urban or rural settlements from a distance but do not know to name them? In this project, we will develop a mobile app that solves this problem by endowing the reality observed by the smart device with proper geographical labels while taking into consideration visibility constraints. Additional features over the basic capacity will be considered during the project. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Securidrone (SE)</h3>
<p>In the drone area, where copters already accomplish a range of tasks from <a href="http://dronebusinessmarketer.com/best-selfie-drone/" role="link">taking selfies</a> to <a href="https://www.amazon.com/Amazon-Prime-Air/b?node=8037720011" role="link">deliver packages</a>, it is only imagination that limits what we can do with such devices. In this project, we will develop a security drone to replace a static surveillance camera system. This drone will possess a certain degree of autonomy in patrolling and securing a predefined property while streaming live video and reporting suspicious activities to the operator. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>From Shape of Drops to Contact lens that doesn’t irritate the eye (CS)</h3>
<p>The shape of a drop bears important information on the surface tension of the drop and its wetting properties. This information can determine if our glasses will become foggy with droplets or how to relate contact lenses to the teardrops of an individual person so that it will not irritate the user’s eyes. The basis for such studies requires software that will recognize a drop image and differentiate between shapes of different drops. In this project, we will work towards automatic extraction and description of the shape of <a href="https://bgu365.sharepoint.com/:i:/s/ICVL/EXniNZV0L-dHl6rslEwiLMEBaRiTFOa0E5nn15eN1ri5yw?e=GCqnyb" role="link">drops that are suspended from a syringe</a>, and based on the shape, calculate the surface tension of the liquid. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a> or <a href="mailto:tadmorr@bgu.ac.il" role="link">Prof. Rafael Tadmor</a> (Mechanical Engineering).</p>

<h3>Eye movements in the wild (CS)</h3>
<p>Humans execute rapid eye movements between locations well over 100,000 times a day. These movements are an integral part of our visual system, used for acquiring relevant information, tracking visual targets, and allowing us to function properly in our world. Tracking eye-movements and measuring how they change in response to different visual stimuli provide a window into human visual perception. Recent portable eye-tracking technology allows measuring eye movements under natural conditions. Most research based on this technology explores the nature of the visual system when it is bound to a specific and explicit task (e.g., driving, walking, or preparing food). But what could eye-movements tell us about the visual system when it is independent of any specific or explicit task? In order to answer this question, we will measure human eye-movement behavior under free viewing conditions in the outdoors using goggles with eye tracking capacities. By analyzing the measurements, we will explore several issues related to the function of the human visual system under natural conditions and will seek ways to incorporate this knowledge into artificial visual systems. For example, we will ask whether or not the bias to move our eyes to the center of the visual field persists even when the head moves freely and the retinal image changes continuously. Several different projects may be available under this topic. <strong>For details please contact:</strong> <a href="mailto:rotem.mairon@gmail.com" role="link">Mr. Rotem Mairon</a> or <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Parkingdrone (SE)</h3>
<p>Another drone-based project, suitable in particular to the BGU campus, we will attempt to develop a system that finds cars who park illegally around campus (or a given parking lot). Unlike BGU’s parking warden, who travels around the parking lot with his electric scooter, we will develop a drone system that does so automatically and registers illegally parking cars based on their windshield permit and the characterization of the parking lot. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>Ize (SE)</h3>
<p>If Waze made ways social, Ize will do so for eyes. In this project, we will develop a mobile application where our eyes are shared for the benefit of all. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>

<h3>A computational generator of geometric visual illusions (CS,SE)</h3>
<p>Optimal illusions are a fascinating phenomenon that captures the imagination and often provides insights into the function of our visual system. In this project, we would like to develop an end-to-end system that generates <a href="https://bgu365.sharepoint.com/:b:/s/ICVL/EYgLPThfeAVIlB9qi4lP5hUB83GDGhmfiQ1nHz0cZ2V8zQ?e=YYCKIO" role="link">topology-disturbing geometric optical illusions of the sort invented by Kokichi Sugihara</a> and popularized in the 2016 best optical illusion contest <a href="https://www.youtube.com/watch?v=oWfFco7K9v8&amp;feature=youtu.be" role="link">and shown here</a>. Such a system will allow users to specify the viewer and mirrors position (and we will even attempt to generalize the illusion to more than one mirror!) and what perceptual shapes are desired, and if those shapes prove to produce the real 3D shape that one needs to generate to obtain the illusions. The system will also convert this shape to a proper representation (i.e., file format) that can be used with the 3D printers in our lab, so the illusions can be constructed in real life. <strong>For details please contact:</strong> <a href="mailto:ben-shahar@cs.bgu.ac.il" role="link">Prof. Ohad Ben-Shahar</a>.</p>
</div>

                    </main>
    </div>
    <div id="footer-placeholder"></div>
    <script src="/ICVL/js/loadContent.js"></script>
</body>
</html>
