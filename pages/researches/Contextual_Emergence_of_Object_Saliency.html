<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contextual Emergence of Object Saliency</title>
    <base href="../../">
    <link rel="stylesheet" href="css/all.css">
</head>
<body>
    <div id="header-placeholder"></div>
    <div class="main-container">
        <main id="content">
<section class="research-section">
    <h2>Contextual Emergence of Object Saliency</h2>

    <p>Visual context is used in different forms for saliency computation. While its use in saliency models for fixations prediction is often reasoned, this is less so the case for approaches that aim to compute saliency at the object level. We argue that the types of context employed by these methods lack clear justification and may in fact interfere with the purpose of capturing the saliency of whole visual objects. In this paper, we discuss the constraints that different types of context impose and suggest a new interpretation of visual context that allows the emergence of saliency for more complex, abstract, or multiple visual objects. Despite shying away from an explicit attempt to capture “objectness” (e.g., via segmentation), our results are qualitatively superior and quantitatively better than the state-of-the-art.</p>

    <div class="image-group">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/1a.jpg" alt="1a" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/1b.jpg" alt="1b" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/2a.jpg" alt="2a" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/2b.jpg" alt="2b" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/3a.jpg" alt="3a" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/3b.jpg" alt="3b" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/4a.jpg" alt="4a" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/4b.jpg" alt="4b" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/5a.jpg" alt="5a" width="116" height="88">
        <img src="./assets/research/Contextual-Emergence-Of-Object-Saliency/5b.jpg" alt="5b" width="116" height="88">
    </div>

    <h2>Download Matlab Code (and additional required software)</h2>
    <ul>
        <li><a href="https://bgu365.sharepoint.com/:u:/s/ICVL/EZyZAW0zYqFFlFtr9jFbvYEBEE6osP1lllUwhFChjEpYkg?e=6Ch95q">Source</a> : Matlab source code of our method for contextual emergence of object saliency. Additional software required to apply the code is available to download in the link below. Please read the included README.txt file for information.</li>
        <li><a href="https://bgu365.sharepoint.com/:u:/s/ICVL/EeyvunfSofRCj6jJDfeGaFQBvmQSsTTEmxxZ4GGjLbF7Rg?e=TaPDgg">Additional required software</a></li>
    </ul>

    <h2>Paper</h2>
    <ul>
        <li>Mairon R. and Ben-Shahar O., <a href="https://bgu365.sharepoint.com/:b:/s/ICVL/EbdXGWKmQKZCrzK9Mj-AnjwBd-gIMsI_eLjt89zwyrqwXQ?e=6QLd7r">A Closer Look at Context: Contextual Emergence of Object Saliency</a>, in the proceedings of the European Conference of Computer Vision (ECCV), 2014.</li>
    </ul>

    <h2>Video</h2>
    <ul>
        <li>The <a href="http://videolectures.net/eccv2014_mairon_object_saliency/">video lecture</a> of the paper presentation in ECCV ’14 is available in videonectures.net</li>
        <li>The video lecture page also includes the presentation slides in .pdf format.</li>
    </ul>

    <h2>Acknowledgments</h2>
    <p>This work was supported in part by the National Institute for Psychobiology in Israel (grant no. 9-2012/2013) founded by the Charles E. Smith Family, by the Israel Science Foundation (ISF grants no. 259/12 and 1274/11), and by the European Commission in the 7th Framework Programme (CROPS GA no. 246252). We also thank the Frankel Fund, the ABC Robotics initiative, and the Zlotowski Center for Neuroscience at Ben-Gurion University for their generous support.</p>
</section>
        </main>
    </div>
    <div id="footer-placeholder"></div>
    <script src="./js/loadContent.js"></script>
</body>
</html>
